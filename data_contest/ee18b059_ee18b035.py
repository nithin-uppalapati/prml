# -*- coding: utf-8 -*-
"""EE18B059_EE18B035.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xB93PmUI22Xg8epk4jVYtxNfUEROUL-G
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import scipy
from sklearn.preprocessing import LabelEncoder
import os
import matplotlib.pyplot as plt

from sklearn import metrics

import scipy
import os


# Metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error


from sklearn.model_selection import GridSearchCV

# from sklearn.preprocessing import Imputer
from sklearn.impute import SimpleImputer
Imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

#tuning hyperparameters
# from bayes_opt import BayesianOptimization
# from skopt  import BayesSearchCV 



import catboost as cb

# import os
# paths = []
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         paths.append(os.path.join(dirname, filename))
# paths

# os.chdir('/kaggle/input/inputdataset/PRML MKN Jan-21 Dataset/')
# # last line

# !ls

train_df = pd.read_csv("train.csv")
song_labels = pd.read_csv("song_labels.csv")
test_df = pd.read_csv("test.csv")
save_df = pd.read_csv("save_for_later.csv")
songs_df = pd.read_csv("songs.csv")
song_labels_df = pd.read_csv("song_labels.csv")
train = pd.read_csv("train.csv")

# import scipy.stats

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory





# os.getcwd()





# print(train_df.shape)
# train_df.head()


# print(song_labels.shape)
# song_labels.head()
# song_labels["platform_id"].unique()


# print(test_df.shape)
# test_df.head()


# ## Save_later.csv



# print(save_df.shape)
# save_df.head()


# ## Songs.csv



# print(songs_df.shape)
# songs_df.head()

# Mean or Mode imputation for NaN values


# print(song_labels_df.shape)
# song_labels_df.head()


# In[14]:


# print(np.nonzero(np.array(train_df.isnull())))
# print(np.nonzero(np.array(test_df.isnull())))
# print(np.nonzero(np.array(save_df.isnull())))
# print(np.nonzero(np.array(songs_df.isnull())))
# print(np.nonzero(np.array(song_labels_df.isnull())))





a = np.empty(len(train))
a[:] = np.NaN
# a



train = train.assign(platform_id=pd.Series(a))
train = train.assign(released_year=pd.Series(a))
train = train.assign(language=pd.Series(a))
train = train.assign(number_of_comments=pd.Series(a))
train = train.assign(mean_customer_rating=pd.Series(a))
train = train.assign(mean_song_rating=pd.Series(a))
#train = train.assign(mean_song_rating=pd.Series(a))
# train.head()



customer_dict = {}
songs_dict = {}



for customer_id in (train["customer_id"].unique()):
    if(customer_id not in customer_dict.keys()):
        customer_dict[customer_id] = np.mean(train.loc[train.customer_id==customer_id,"score"])



for song_id in (train["song_id"].unique()):
    if(song_id not in songs_dict.keys()):
        songs_dict[song_id] = np.mean(train.loc[train.song_id==song_id,"score"])




for customer_id in (train["customer_id"].unique()):
    train.loc[train.customer_id == customer_id,"mean_customer_rating"] = customer_dict[customer_id]




for song_id in (train["song_id"].unique()):
    train.loc[train.song_id == song_id,"mean_song_rating"] = songs_dict[song_id]



for song_id in list(songs_df["song_id"]):
    train.loc[train.song_id==song_id,"platform_id"] = np.array(songs_df.loc[songs_df.song_id==song_id,"platform_id"])[0]    
    train.loc[train.song_id==song_id,"released_year"] = np.array(songs_df.loc[songs_df.song_id==song_id,"released_year"])[0]     
    train.loc[train.song_id==song_id,"language"] = np.array(songs_df.loc[songs_df.song_id==song_id,"language"])[0]    
    train.loc[train.song_id==song_id,"number_of_comments"] = np.array(songs_df.loc[songs_df.song_id==song_id,"number_of_comments"])[0] 


train["language"].fillna("eng",inplace=True)
train["released_year"].fillna(np.mean(train["released_year"]),inplace=True)
train["number_of_comments"].fillna(np.mean(train["number_of_comments"]),inplace=True)


l1 = LabelEncoder()
l2 = LabelEncoder()



l1.fit(train["language"])
a = l1.transform(train["language"])



train["language"] = a


# In[34]:


l2.fit(train["customer_id"])
b = l2.transform(train["customer_id"])


train["customer_id"] = b



train.loc[train["released_year"] < 1500,"released_year"] = np.mean(list(train.loc[train["released_year"] > 1500]["released_year"]))



test = pd.read_csv("test.csv")



a = np.empty(len(test))
a[:] = np.NaN
# a



test = test.assign(platform_id=pd.Series(a))
test = test.assign(released_year=pd.Series(a))
test = test.assign(language=pd.Series(a))
test = test.assign(number_of_comments=pd.Series(a))
test = test.assign(mean_customer_rating=pd.Series(a))
test = test.assign(mean_song_rating=pd.Series(a))




for song_id in list(songs_df["song_id"]):
    test.loc[test.song_id==song_id,"platform_id"] = np.array(songs_df.loc[songs_df.song_id==song_id,"platform_id"])[0]    
    test.loc[test.song_id==song_id,"released_year"] = np.array(songs_df.loc[songs_df.song_id==song_id,"released_year"])[0]     
    test.loc[test.song_id==song_id,"language"] = np.array(songs_df.loc[songs_df.song_id==song_id,"language"])[0]    
    test.loc[test.song_id==song_id,"number_of_comments"] = np.array(songs_df.loc[songs_df.song_id==song_id,"number_of_comments"])[0]   



for customer_id in (test["customer_id"].unique()):
    test.loc[test.customer_id == customer_id,"mean_customer_rating"] = customer_dict[customer_id]



for song_id in (test["song_id"].unique()):
    test.loc[test.song_id == song_id,"mean_song_rating"] = songs_dict[song_id]



test.drop(["platform_id"],axis=1,inplace=True)



test["language"].fillna("eng",inplace=True)



test["number_of_comments"].fillna(np.mean(train["number_of_comments"]),inplace=True)
test["released_year"].fillna(np.mean(train["released_year"]),inplace=True)


c = l1.transform(test["language"])

test["language"] = c


d = l2.transform(test["customer_id"])


test["customer_id"] = d



test.loc[test["released_year"] < 1500,"released_year"] = np.mean(list(test.loc[test["released_year"] > 1500]["released_year"]))



# test.head()

# train.head()

# train.head()
train_temp = train

"""# New Section"""

train = train_temp.drop(['platform_id'], axis=1)
# train.head()

# train.head()

# test.head()

# train = pd.concat([train_x, train_y], axis=1)
# train.head()





n_users = train.customer_id.unique().shape[0]
n_items = train.song_id.unique().shape[0]

# print(n_users,n_items)
a=train.customer_id.unique()
b=a.copy()
b.sort()
# print(b)

a=train.song_id.unique()
b=a.copy()
b.sort()
# print(b)

# train.head()

train_x = train.drop(["score"],axis=1)
# train_x.head()

train_y = train["score"]
# train_y.head()

data_matrix = np.zeros((n_users, n_items))
for line in train.itertuples():
    data_matrix[line[1], line[2]-1] = line[8]

# print(data_matrix)
# print(data_matrix.shape)

r = data_matrix !=0
r = r+0
# r

"""Catboost Notes:

1. Add a feature based on user similarity matrix's weighted mean of score (user based, and item based)
"""

song_popularity = r.sum(axis=0)

# song_popularity.shape

song_popularity = song_popularity/(song_popularity.max())
# song_popularity

# print(song_popularity.min())
# print(song_popularity.max())

# song_popularity

train_x["song_popularity"] = 0
test["song_popularity"] = 0
# train_x

to_df_song_popularity = []
for line in train_x.itertuples():
    to_df_song_popularity.append(song_popularity[line[2]-1])
train_x["song_popularity"] = to_df_song_popularity

to_df_song_popularity = []
for line in test.itertuples():
    to_df_song_popularity.append(song_popularity[line[2]-1])

test["song_popularity"] = to_df_song_popularity

# train_x

# test

# my_model = cb.CatBoostRegressor(iterations = 1000, early_stopping_rounds=100, loss_function='RMSE', learning_rate = 0.01)#, task_type = "GPU")#, devices='0')

# parameters = { 'iterations' : [1000],
#               'early_stopping_rounds' : [100],
#               'learning_rate' : [0.01],
#                 'depth': [6,11],
#                 'l2_leaf_reg': [0.5, 1, 3, 5, 8, 10],
#                 'cat_features': [[0,1,3]],
#               'loss_function' : ['RMSE'],
#               'bagging_temperature' : [0, 0.1, 0.2, 0.3, 0.4]#, 0.5, 0.6, 0.7, 0.8, 0.9 ]
#               }

# gridsearch = GridSearchCV(estimator = my_model, param_grid = parameters, cv = 5)
# gridsearch.fit(train_x, train_y)

# dict1=gridsearch.best_params_

# # {'cat_features': [0, 1, 2],
# #  'depth': 6,
# #  'iterations': 1000,
# #  'l2_leaf_reg': 0.5,   1
# #  'learning_rate': 0.1}

# dict1

# # '''

# # OUR PREVIOUS PARAMS

# # {'cat_features': [0, 1, 3],
# #  'depth': 10,
# #  'early_stopping_rounds': 100,
# #  'iterations': 1000,
# #  'l2_leaf_reg': 0.2,
# #  'learning_rate': 0.01,
# #  'loss_function': 'RMSE'}
 
# # '''



# ##### with cross validation = 5. These are best.
# # {'cat_features': [0, 1, 3],
# #  'depth': 10,
# #  'early_stopping_rounds': 100,
# #  'iterations': 1000,
# #  'l2_leaf_reg': 0.2,
# #  'learning_rate': 0.01,
# #  'loss_function': 'RMSE'}

# cb.CatBoostRegressor(iterations = 1000, early_stopping_rounds=100, loss_function='RMSE', learning_rate = 0.01)

# test.head()

# train_x.head()

# train_y.head()

model = cb.CatBoostRegressor(iterations = 1000, early_stopping_rounds=100 ,loss_function='RMSE', learning_rate = 0.01 , depth = 10 , l2_leaf_reg = 0.2 , cat_features = [0,1,3])#, bagging_temperature = dict1['bagging_temperature'])
model.fit(train_x, train_y)
#predict target
y_pred = model.predict(test)
# train_pred = model.predict(train_x)

lst2 = y_pred.tolist()
lst1 = []
for i in range(len(lst2)):
    lst1.append(i)
dfsubmit = pd.DataFrame(np.array(lst1),columns=['test_row_id'])
dfsubmit['score'] = lst2
dfsubmit.to_csv('final_output.csv', index = False, header = True)

# y_pred

# lst2 = y_pred.tolist()
# lst1 = []
# for i in range(len(lst2)):
#     lst1.append(i)
# dfsubmit = pd.DataFrame(np.array(lst1),columns=['test_row_id'])
# dfsubmit['score'] = lst2
# dfsubmit.to_csv('./catboost_results_v2_june1st.csv', index = False, header = True)
# train_pred = model.predict(train_x)

# train_pred

########################
# model.fit(train_x, train_y)
#predict target
# y_pred_train = model.predict(train_x)

# mse = mean_squared_error(train_pred,train_y)

# mse

# 0.75428 - 0.6726



# Experimentation AREA!!

